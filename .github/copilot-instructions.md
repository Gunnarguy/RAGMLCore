# GitHub Copilot Instructions for RAGMLCore

## Project Overview

RAGMLCore is an **on-device RAG (Retrieval-Augmented Generation) application for iOS 18.1+ and iOS 26**. The app implements a complete RAG pipeline supporting multiple AI pathways: Apple's Foundation Models (on-device + Private Cloud Compute), custom Core ML models, and optional ChatGPT integration.

**Current Status (October 2025)**: iOS 26 IS RELEASED with full Apple Intelligence framework support.

**Key Principle**: Privacy-first architecture with on-device processing by default, Private Cloud Compute for complex queries (Apple Silicon servers, zero data retention).

---

## Architecture Essentials

### Protocol-Based Design (Critical for Extensions)

Every major component uses protocol abstraction to enable swapping implementations:

```swift
// Services/LLMService.swift - FOUR implementations coexist
protocol LLMService {
    func generate(prompt: String, context: String?, config: InferenceConfig) async throws -> LLMResponse
    var isAvailable: Bool { get }
    var modelName: String { get }
}

// 1. AppleFoundationLLMService - iOS 26 on-device Foundation Models (Pathway A)
// 2. PrivateCloudComputeService - Apple's PCC for complex queries (Pathway A+)
// 3. CoreMLLLMService - Custom .mlpackage models (Pathway B1)
// 4. MockLLMService - Testing without real models
```

**When adding features**: Maintain protocol conformance. Test with `MockLLMService` first.

### Apple Intelligence API Landscape (iOS 18.1+ / iOS 26)

**Available NOW**:

- **Foundation Models framework** - On-device ~3B param model (LanguageModelSession, SystemLanguageModel)
- **Private Cloud Compute (PCC)** - Automatic fallback to Apple Silicon servers for complex queries
- **Writing Tools API** - Proofreading, rewriting, summarization (integrates with any text field)
- **App Intents framework** - Siri integration for custom RAG queries
- **Image Playground API** - On-device image generation (Animation, Illustration, Sketch styles)
- **ChatGPT Integration** - Optional third-party model (requires user OpenAI account)
- **Core ML** - Custom model execution with Neural Engine optimization### RAG Pipeline Data Flow

```
Document Import ‚Üí Parse (PDFKit) ‚Üí Chunk (400w/50w overlap) ‚Üí
Embed (NLEmbedding 512-dim) ‚Üí Store (VectorDatabase) ‚Üí
Query Embed ‚Üí Cosine Similarity Search (top-k) ‚Üí
Format Context ‚Üí LLM Generate ‚Üí Response + Metrics
```

**Orchestrated by**: `RAGService.swift` (single source of truth, ObservableObject for UI reactivity)

**Key methods**:

- `addDocument(at:)` - Ingestion pipeline
- `query(_:topK:)` - RAG query execution

### File Organization

```
RAGMLCore/
‚îú‚îÄ‚îÄ Models/              # Pure data structures (Codable, no logic)
‚îÇ   ‚îú‚îÄ‚îÄ DocumentChunk.swift    # Contains embedding: [Float] (512-dim)
‚îÇ   ‚îú‚îÄ‚îÄ LLMModel.swift         # InferenceConfig has NO explicit init (main actor isolation)
‚îÇ   ‚îî‚îÄ‚îÄ RAGQuery.swift         # RAGResponse includes performance metadata
‚îÇ
‚îú‚îÄ‚îÄ Services/            # ‚ö†Ô∏è Enhancement work area
‚îÇ   ‚îú‚îÄ‚îÄ DocumentProcessor.swift   # Semantic chunking with paragraph boundaries
‚îÇ   ‚îú‚îÄ‚îÄ EmbeddingService.swift    # NLEmbedding (NOT NLContextualEmbedding - deprecated)
‚îÇ   ‚îú‚îÄ‚îÄ VectorDatabase.swift      # Protocol + InMemoryVectorDatabase impl
‚îÇ   ‚îú‚îÄ‚îÄ LLMService.swift          # Protocol + 3 implementations
‚îÇ   ‚îî‚îÄ‚îÄ RAGService.swift          # Orchestrator with @Published state
‚îÇ
‚îî‚îÄ‚îÄ Views/               # SwiftUI, observes RAGService
    ‚îú‚îÄ‚îÄ ChatView.swift           # Uses iOS 17+ onChange (zero-param closure)
    ‚îú‚îÄ‚îÄ DocumentLibraryView.swift
    ‚îî‚îÄ‚îÄ ModelManagerView.swift
```

---

## Development Workflow

### Building & Testing

```bash
# Open in Xcode
open RAGMLCore.xcodeproj

# Build: ‚åò + B (all Swift files should compile with zero errors)
# Run: ‚åò + R (uses MockLLMService until iOS 26 SDK available)
```

**Manual testing checklist**:

1. Documents tab ‚Üí Import PDF ‚Üí Verify "Processing..." overlay
2. Chat tab ‚Üí Type query ‚Üí Verify mock response with retrieved chunks
3. Models tab ‚Üí Check device capabilities display

### Code Conventions

**Async/await everywhere** (no completion handlers):

```swift
// ‚úÖ Correct
func processDocument(_ url: URL) async throws -> [DocumentChunk] {
    let text = try await extractText(from: url)
    return await chunkText(text)
}

// ‚ùå Avoid blocking
func processDocument(_ url: URL) throws -> [DocumentChunk] {
    sleep(2) // Blocks UI thread!
    return chunks
}
```

**@MainActor for UI updates**:

```swift
@MainActor
class RAGService: ObservableObject {
    @Published var documents: [Document] = []
    // Updates automatically trigger SwiftUI redraws
}
```

**Keep functions under 50 lines**. Add comments for complex algorithms (e.g., `averageEmbeddings()` in `EmbeddingService.swift`).

### Enhancement Markers

Use `// TODO: Optional Enhancement` for features that could be added:

```swift
// TODO: Optional Enhancement - Implement BPE tokenizer
func tokenize(_ text: String) -> [Int] {
    fatalError("Tokenizer not yet implemented")
}
```

---

## Critical Implementation Details

### Embedding Service (NLEmbedding API)

**Updated in current SDK** - do NOT use `NLContextualEmbedding`:

```swift
// ‚úÖ Current API (Services/EmbeddingService.swift)
private let embedder = NLEmbedding.wordEmbedding(for: .english)

func generateEmbedding(for text: String) async -> [Float] {
    let words = text.split(separator: " ")
    let vectors = words.compactMap { embedder?.vector(for: String($0)) }
    return averageEmbeddings(vectors) // Word-level averaging for chunk representation
}
```

### iOS 26 Foundation Models (NOW AVAILABLE)

`AppleFoundationLLMService` uses **released iOS 26 APIs**:

```swift
// LLMService.swift - UNCOMMENT lines 100-110 to enable
import FoundationModels

@available(iOS 26.0, *)
class AppleFoundationLLMService: LLMService {
    private var session: LanguageModelSession?

    var isAvailable: Bool {
        return SystemLanguageModel.isAvailable
    }

    init() {
        if isAvailable {
            self.session = LanguageModelSession(
                instructions: "You are a helpful RAG assistant..."
            )
        }
    }

    func generate(...) async throws -> LLMResponse {
        guard let session = session else { throw LLMError.modelUnavailable }

        // Use LanguageModelSession.generate() for on-device inference
        // Automatically falls back to Private Cloud Compute for complex queries
        let response = try await session.generate(prompt: augmentedPrompt)
        return LLMResponse(text: response.text, ...)
    }
}
```

**IMMEDIATE ACTION**: Update `RAGService.init()` line ~32 to use `AppleFoundationLLMService` instead of `MockLLMService`.

### Private Cloud Compute (Apple's Cloud Inference)

**New in iOS 26** - Automatic hybrid on-device/cloud inference:

```swift
// Services/PrivateCloudComputeService.swift - NEW implementation
@available(iOS 26.0, *)
class PrivateCloudComputeService: LLMService {
    private var session: LanguageModelSession?

    var isAvailable: Bool {
        return SystemLanguageModel.isAvailable
    }

    init() {
        self.session = LanguageModelSession(
            instructions: "...",
            preferredExecutionContext: .cloud // Force PCC for complex queries
        )
    }

    // PCC Benefits:
    // - Runs on Apple Silicon servers (same architecture as device)
    // - Zero data retention (cryptographically enforced)
    // - Seamless fallback from on-device when needed
    // - Higher token limits and faster inference for complex queries
}
```

**Use Cases**:

- Long document summarization (>10k tokens)
- Complex multi-step reasoning
- Queries requiring more than ~3B parameter capacity
- User preference for higher quality responses

### ChatGPT Integration (Optional Third-Party)

**Available in iOS 18.1+** - Seamlessly integrated with Apple Intelligence:

```swift
// Services/ChatGPTService.swift - Optional implementation
class ChatGPTService: LLMService {
    var isAvailable: Bool {
        // User must enable ChatGPT in Settings > Apple Intelligence
        return ChatGPT.isAvailable
    }

    func generate(...) async throws -> LLMResponse {
        // User consents before ANY data sent to OpenAI
        // Provides access to GPT-4 and web-connected queries
        let request = ChatGPTRequest(prompt: augmentedPrompt)
        let response = try await ChatGPT.send(request)
        return LLMResponse(text: response.text, ...)
    }
}
```

**Privacy Note**: Requires explicit user consent per-request. Data sent to OpenAI (not Apple). Free tier available without account.

### Writing Tools API (System-Wide)

**Available NOW** - Integrate Apple's proofreading/rewriting into your app:

```swift
import WritingTools

// In any text field or editor
.writingToolsEnabled(true) // Enable system Writing Tools
.onWritingToolsAction { action in
    switch action {
    case .proofread(let corrected):
        // Apply corrections
    case .rewrite(let alternatives):
        // Show alternatives
    case .summarize(let summary):
        // Use in RAG context preparation
    }
}
```

**Use in RAG**: Summarize retrieved chunks before passing to LLM for more efficient context.

### App Intents (Siri Integration)

**Enable RAG queries via Siri**:

```swift
import AppIntents

struct QueryDocumentsIntent: AppIntent {
    static var title: LocalizedStringResource = "Query Documents"

    @Parameter(title: "Question")
    var question: String

    func perform() async throws -> some IntentResult {
        let ragService = RAGService()
        let response = try await ragService.query(question)
        return .result(value: response.generatedResponse)
    }
}
```

**Usage**: "Hey Siri, query my documents about quarterly revenue"

### Vector Database Abstraction

**Current**: `InMemoryVectorDatabase` (data lost on restart)  
**Optional Enhancement**: Replace with `VecturaVectorDatabase` (persistent HNSW storage)

```swift
// VectorDatabase.swift - Protocol enables swapping
protocol VectorDatabase {
    func store(chunk: DocumentChunk) async throws
    func search(embedding: [Float], topK: Int) async throws -> [RetrievedChunk]
}

// Optional: Add VecturaKit SPM dependency, implement VecturaVectorDatabase
```

### Chunking Strategy

**Semantic paragraph-based splitting** with 50-word overlap for context preservation:

```swift
// DocumentProcessor.swift - chunkText()
// Targets 400 words per chunk, splits on paragraph boundaries
// Overlap ensures no information loss at chunk edges
```

---

## Optional Enhancement Guide

### Next Priority Tasks

1. **Enable Apple Foundation Models** (~2 hours)

   - Uncomment `AppleFoundationLLMService` implementation
   - Update `RAGService.init()` to use real model
   - Test on A17 Pro+ or M-series device

2. **Add Private Cloud Compute fallback** (~4-6 hours)

   - Implement `PrivateCloudComputeService` with `.cloud` execution context
   - Add user preference toggle (on-device vs cloud)
   - Test with complex multi-document queries

3. **Persistent Vector Database** (~4-8 hours)

   - Add VecturaKit via SPM
   - Implement `VecturaVectorDatabase: VectorDatabase`
   - Replace initialization in `RAGService.init()`

4. **ChatGPT Integration** (~2-4 hours)

   - Implement `ChatGPTService` using Apple's ChatGPT framework
   - Add consent UI for OpenAI data sharing
   - Add model selection: Foundation Models vs ChatGPT

5. **Writing Tools Integration** (~2-3 hours)

   - Enable Writing Tools in chat input field
   - Add "Summarize Context" button using Writing Tools API
   - Integrate proofreading for user queries

6. **App Intents for Siri** (~3-4 hours)

   - Create `QueryDocumentsIntent` for Siri queries
   - Add `AddDocumentIntent` for voice-based import
   - Test with Siri shortcuts integration

7. **Custom Model Tokenizer** (~8-16 hours)

   - Choose: swift-transformers or custom BPE
   - Implement in `CoreMLLLMService` (lines 162-220)

8. **Autoregressive Generation Loop** (~16-24 hours)
   - Complete `CoreMLLLMService.generate()`
   - Add KV-cache state management
   - Implement temperature sampling

**Alternative**: GGUF pathway via llama.cpp integration (40-50 hours, recommended for user flexibility)

### Testing Strategy

**Unit tests** (protocol-based, use mocks):

```swift
func testEmbeddingDimensionality() async throws {
    let embedding = await embeddingService.generateEmbedding(for: "test")
    XCTAssertEqual(embedding.count, 512) // NLEmbedding fixed dimensionality
}
```

**Integration tests**:

```swift
func testEndToEndRAGPipeline() async throws {
    try await ragService.addDocument(at: testPDFURL)
    let response = try await ragService.query("What is the topic?")
    XCTAssertGreaterThan(response.retrievedChunks.count, 0)
}
```

---

## Documentation Navigation

**Before making changes, consult**:

| Question                   | File                       | Lines/Section        |
| -------------------------- | -------------------------- | -------------------- |
| What's the current status? | `IMPLEMENTATION_STATUS.md` | Executive Summary    |
| How do I implement X?      | `ENHANCEMENTS.md`          | Code examples        |
| Why this architecture?     | `ARCHITECTURE.md`          | Design decisions     |
| How does component Y work? | `ARCHITECTURE.md`          | Core Components      |
| What's next to build?      | `IMPLEMENTATION_STATUS.md` | Next Steps section   |
| How do I contribute?       | `CONTRIBUTING.md`          | Incremental workflow |

**After making changes, update**:

- `IMPLEMENTATION_STATUS.md` - Progress percentages
- `ARCHITECTURE.md` - If you changed design patterns
- `ENHANCEMENTS.md` - If you completed an optional enhancement

---

## Common Pitfalls & Solutions

### ‚ùå "Missing Combine import" Error

```swift
// RAGService.swift MUST import Combine for ObservableObject
import Foundation
import Combine  // Required for @Published
```

### ‚ùå "Main actor isolated property" Error

```swift
// InferenceConfig should NOT have explicit init (causes main actor issues)
struct InferenceConfig {
    var temperature: Float = 0.7
    var maxTokens: Int = 500
    // Use compiler-generated memberwise init
}
```

### ‚ùå "Deprecated onChange API" Warning

```swift
// ‚úÖ iOS 17+ syntax (ChatView.swift)
.onChange(of: ragService.messages) {
    scrollToBottom() // Zero-parameter closure
}

// ‚ùå Old syntax
.onChange(of: ragService.messages) { newValue in ... }
```

### ‚ùå "Model prediction not awaited" Error

```swift
// CoreMLLLMService.swift - Core ML prediction is async
let output = try await model.prediction(from: input) // Must await
```

---

## Key Design Decisions (Context for AI Agents)

### Why Protocol Abstraction?

**Problem**: Need to support 3+ LLM backends (Foundation Models, Core ML, GGUF) without code duplication.

**Solution**: `LLMService` protocol enables runtime switching based on device capabilities and user preference.

**Example**: `RAGService` never knows which implementation it's using - just calls `llmService.generate()`.

### Why Word-Level Embedding Averaging?

**Problem**: NLEmbedding provides word-level vectors, but we need chunk-level representations.

**Solution**: Average all word vectors in a chunk (see `EmbeddingService.averageEmbeddings()`). Simple but effective for semantic similarity.

**Optional Enhancement**: Use swift-embeddings for model-matched embeddings.

### Why 400-Word Chunks with 50-Word Overlap?

**Problem**: Balance between semantic coherence and retrieval granularity.

**Solution**: 400 words ‚âà 1-2 paragraphs (semantic unit). 50-word overlap prevents context loss at boundaries.

**Configurable**: `DocumentProcessor(targetChunkSize:chunkOverlap:)` constructor.

### Why In-Memory Vector Database?

**Current Implementation Goal**: Validate RAG logic without external dependencies.

**Production Enhancement**: Replace with VecturaKit (HNSW index, persistent storage, hybrid search).

**Pattern**: Protocol abstraction means UI/business logic unchanged when swapping implementations.

---

## Performance Expectations

| Operation                   | Target         | Current Status                        |
| --------------------------- | -------------- | ------------------------------------- |
| Document parsing            | <1s/page       | ‚úÖ Achieved                           |
| Embedding generation        | <100ms/chunk   | ‚úÖ Achieved                           |
| Vector search (1000 chunks) | <50ms          | ‚úÖ In-memory (will improve with HNSW) |
| LLM generation (on-device)  | 10+ tokens/sec | ‚úÖ Foundation Models available        |
| LLM generation (PCC)        | 20+ tokens/sec | ‚úÖ Private Cloud Compute available    |
| End-to-end query            | <5s            | ‚úÖ Ready for production               |

**Profile with Instruments** if adding computationally expensive features.

---

## External Dependencies

| Dependency         | Purpose                  | Status                  |
| ------------------ | ------------------------ | ----------------------- |
| FoundationModels   | Apple's ~3B param LLM    | ‚úÖ iOS 26 available     |
| Private Cloud      | PCC fallback inference   | ‚úÖ iOS 26 available     |
| ChatGPT            | Optional third-party LLM | ‚úÖ iOS 18.1+ available  |
| Writing Tools      | System proofreading/edit | ‚úÖ iOS 18.1+ available  |
| App Intents        | Siri integration         | ‚úÖ Available            |
| Core ML            | Custom model execution   | ‚úÖ Available            |
| NaturalLanguage    | Embedding generation     | ‚úÖ Using NLEmbedding    |
| PDFKit             | Document parsing         | ‚úÖ Integrated           |
| VecturaKit         | Persistent vector DB     | üìã Optional Enhancement |
| swift-transformers | Tokenizer                | üìã Optional Enhancement |

**Add new dependencies via Swift Package Manager** in Xcode: File ‚Üí Add Package Dependencies.

---

## Security & Privacy Considerations

1. **On-device first** - Foundation Models run locally, no network calls by default
2. **Private Cloud Compute** - When using PCC, data processed on Apple Silicon servers with cryptographic zero-retention guarantee
3. **ChatGPT consent** - User must explicitly approve EVERY query sent to OpenAI (data leaves Apple ecosystem)
4. **Sandboxed file access** - Use `SecurityScopedResource` pattern (see `DocumentProcessor.swift`)
5. **User consent for documents** - Document picker handles permissions
6. **No analytics/telemetry** - Maintain zero data collection policy
7. **Verifiable privacy** - PCC architecture allows independent security research to verify Apple's privacy claims

---

## Summary for Quick Context

**What we're building**: Privacy-first on-device RAG app with Apple Intelligence integration  
**iOS 26 Status**: RELEASED (October 2025) - All Apple Intelligence APIs available  
**Current Status**: Core features complete (100%), ready for production deployment  
**Architecture pattern**: Protocol-oriented, async/await, single source of truth  
**Key files for immediate deployment**: `Services/AppleFoundationLLMService.swift`, `Services/PrivateCloudComputeService.swift`  
**Apple Intelligence Features**: Foundation Models, Private Cloud Compute, ChatGPT, Writing Tools, App Intents  
**Testing approach**: Can use MockLLMService for testing, AppleFoundationLLMService for production  
**Documentation**: 7 comprehensive files covering architecture, implementation, and Apple Intelligence integration

**Before coding**: Read `IMPLEMENTATION_STATUS.md` to understand current state  
**While coding**: Follow protocol patterns, use async/await, maintain <50 line functions  
**After coding**: Update docs, test on A17 Pro+ or M-series device, verify zero compilation errors

---

_Last Updated: October 2025_  
_iOS 26 Status: RELEASED with full Apple Intelligence support_  
_Project Version: v0.1.0 (Ready for Production Deployment)_
