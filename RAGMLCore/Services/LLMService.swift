//
//  LLMService.swift
//  RAGMLCore
//
//  Created by Gunnar Hostetler on 10/9/25.
//

import Foundation

/// Protocol defining the interface for LLM inference engines
/// This abstraction enables switching between Foundation Models and Core ML
protocol LLMService {
    /// Generate a response given a prompt and optional context
    func generate(prompt: String, context: String?, config: InferenceConfig) async throws -> LLMResponse
    
    /// Check if the service is available on the current device
    var isAvailable: Bool { get }
    
    /// Get the name of the model being used
    var modelName: String { get }
}

/// Response from an LLM generation request
struct LLMResponse {
    let text: String
    let tokensGenerated: Int
    let timeToFirstToken: TimeInterval?
    let totalTime: TimeInterval
    
    var tokensPerSecond: Float? {
        guard totalTime > 0 else { return nil }
        return Float(tokensGenerated) / Float(totalTime)
    }
}

import NaturalLanguage

// MARK: - Apple Foundation Models (iOS 26+) - REAL Apple Intelligence LLM
// This is Apple's on-device language model with automatic Private Cloud Compute fallback
// Announced at WWDC 2025 - sessions 286, 301, 259
// Requires iOS 26.0+, A17 Pro / M1 or later
// Zero data retention, end-to-end encrypted when using Private Cloud Compute

#if canImport(FoundationModels)
import FoundationModels

@available(iOS 26.0, *)
class AppleFoundationLLMService: LLMService {
    
    private var session: LanguageModelSession?
    private let model: SystemLanguageModel
    
    var isAvailable: Bool {
        return model.isAvailable
    }
    
    var modelName: String {
        return "Apple Foundation Model (On-Device)"
    }
    
    init() {
        // Use the default system language model
        self.model = SystemLanguageModel.default
        
        guard model.isAvailable else {
            print("âš ï¸  Apple Foundation Models not available on this device")
            print("   ðŸ’¡ Requires iOS 26.0+, A17 Pro / M1 or later")
            print("   ðŸ’¡ Apple Intelligence must be enabled in Settings")
            return
        }
        
        // Create language model session with hybrid RAG+LLM instructions
        // This enables BOTH document-based RAG and general conversational AI
        self.session = LanguageModelSession(
            model: model,
            tools: [],
            instructions: Instructions("""
                You are a helpful and friendly AI assistant for a document retrieval system.
                
                When the user provides document context:
                - Analyze the documents and answer questions based on the content
                - Cite specific information when relevant
                - If the documents don't contain the answer, say so clearly
                
                When chatting without documents:
                - Engage naturally and helpfully
                - Answer questions to the best of your ability
                - Be conversational and informative
                - Respond to greetings, tests, and casual conversation naturally
                
                For simple inputs like "test", "hello", or single words, respond conversationally to \
                confirm you're working. Always be helpful and never refuse to respond unless the \
                request is genuinely harmful or inappropriate.
                """)
        )
        
        print("âœ… Apple Foundation Model initialized")
        print("   ðŸ§  Hybrid RAG+LLM mode enabled")
        print("   ðŸ“š RAG mode when documents available")
        print("   ðŸ’¬ General chat mode when no documents")
        print("   ðŸ”’ Zero data retention, end-to-end encrypted")
        print("   ðŸ“ Model: SystemLanguageModel.default")
    }
    
    func generate(prompt: String, context: String?, config: InferenceConfig) async throws -> LLMResponse {
        guard let session = session else {
            throw LLMError.modelUnavailable
        }
        
        let startTime = Date()
        
        print("\nâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
        print("â•‘ APPLE FOUNDATION MODEL GENERATION                            â•‘")
        print("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
        
        // Construct augmented prompt with RAG context
        let fullPrompt: String
        if let context = context, !context.isEmpty {
            print("ðŸ“š MODE: RAG (Document Context Available)")
            print("ðŸ“„ Context length: \(context.count) characters")
            print("â“ User prompt: \(prompt)")
            fullPrompt = """
            Context from user's documents:
            
            \(context)
            
            User question: \(prompt)
            
            Please answer based on the provided context above.
            """
        } else {
            print("ðŸ’¬ MODE: General Chat (No Document Context)")
            print("â“ User prompt: \(prompt)")
            fullPrompt = prompt
        }
        
        print("\nâ”â”â” LLM Configuration â”â”â”")
        print("ðŸŒ¡ï¸  Temperature: \(config.temperature)")
        print("ðŸŽ¯ Max tokens: \(config.maxTokens)")
        
        // Generate response using streaming API
        var responseText = ""
        var tokenCount = 0
        var firstTokenTime: TimeInterval?
        
        let options = GenerationOptions(
            temperature: Double(config.temperature)
        )
        
        print("\nâ”â”â” Starting Generation â”â”â”")
        print("â±ï¸  Start time: \(startTime)")
        
        let responseStream = try session.streamResponse(to: fullPrompt, options: options)
        
        print("ðŸ“¡ Streaming response from Foundation Model...")
        print("ðŸ“¡ Waiting for response snapshots...\n")
        
        var snapshotCount = 0
        var lastContentLength = 0
        
        for try await snapshot in responseStream {
            snapshotCount += 1
            
            if firstTokenTime == nil {
                firstTokenTime = Date().timeIntervalSince(startTime)
                print("âš¡ First token received after \(String(format: "%.2f", firstTokenTime!))s")
            }
            
            // Update response text from snapshot
            let previousLength = responseText.count
            responseText = snapshot.content
            let newChars = responseText.count - previousLength
            
            // More accurate token counting based on word boundaries
            let currentWords = responseText.split(separator: " ").count
            let previousWords = tokenCount
            let newWords = currentWords - previousWords
            
            if newWords > 0 {
                tokenCount = currentWords
            }
            
            // Detailed logging for EVERY snapshot
            print("ðŸ“¦ Snapshot #\(snapshotCount):")
            print("   Snapshot type: \(type(of: snapshot))")
            print("   Content length: \(responseText.count) chars (\(newChars) new)")
            print("   Word count: \(currentWords) words (\(newWords) new)")
            print("   Full content so far:")
            print("   \"\(responseText)\"")
            print("   ---")
            
            lastContentLength = responseText.count
        }
        
        print("\nðŸ“Š Stream Statistics:")
        print("   Total snapshots: \(snapshotCount)")
        print("   Final content length: \(responseText.count) chars")
        print("   Final word count: \(tokenCount) words")
        
        let totalTime = Date().timeIntervalSince(startTime)
        
        // Final token count is accurate word count
        let finalTokenCount = responseText.split(separator: " ").count
        
        print("\nâ”â”â” Generation Complete â”â”â”")
        print("âœ… Total words: \(finalTokenCount)")
        print("âœ… Total characters: \(responseText.count)")
        print("â±ï¸  Total time: \(String(format: "%.2f", totalTime))s")
        if let ttft = firstTokenTime {
            print("âš¡ Time to first token: \(String(format: "%.2f", ttft))s")
        }
        if totalTime > 0 {
            let tps = Float(finalTokenCount) / Float(totalTime)
            print("ðŸš€ Speed: \(String(format: "%.1f", tps)) words/sec")
        }
        
        print("\nðŸ“„ Full Response Text:")
        print("â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
        print(responseText)
        print("â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
        
        print("\nðŸ” Response Analysis:")
        print("   Is response empty? \(responseText.isEmpty)")
        print("   Starts with refusal? \(responseText.lowercased().contains("can't assist") || responseText.lowercased().contains("cannot assist") || responseText.lowercased().contains("apologize"))")
        
        return LLMResponse(
            text: responseText,
            tokensGenerated: finalTokenCount,
            timeToFirstToken: firstTokenTime,
            totalTime: totalTime
        )
    }
}

// NOTE: Private Cloud Compute (PCC) is AUTOMATIC in Foundation Models
// The system intelligently decides when to use on-device vs. Apple's PCC servers
// based on query complexity and available resources. PCC provides:
// - Apple Silicon servers (same architecture as device)  
// - Cryptographic zero-retention guarantee
// - End-to-end encryption
// - Seamless fallback for complex queries
// You don't need a separate service - it's built into AppleFoundationLLMService above

#endif

// MARK: - On-Device Document Analysis (NaturalLanguage Framework)
// FALLBACK for devices without Foundation Models support
// This is NOT an LLM - it's an extractive QA system using Apple's NaturalLanguage framework
// It analyzes your query, finds relevant sentences from retrieved context, and presents them
// This runs 100% on-device with zero network calls and zero AI model downloads

class OnDeviceAnalysisService: LLMService {
    
    private let tagger = NLTagger(tagSchemes: [.lexicalClass, .nameType, .lemma])
    private let languageRecognizer = NLLanguageRecognizer()
    
    var isAvailable: Bool { true }
    var modelName: String { "On-Device Analysis (Extractive QA)" }
    
    init() {
        print("âœ… On-Device Analysis Service initialized")
        print("   ðŸ“ 100% local processing using NaturalLanguage framework")
        print("   ðŸ”’ Zero network calls, zero data collection")
    }
    
    func generate(prompt: String, context: String?, config: InferenceConfig) async throws -> LLMResponse {
        let startTime = Date()
        
        // Simulate analysis time
        let words = prompt.split(separator: " ").count
        let analysisTime = Double(words) / 100.0 + 0.3
        try await Task.sleep(nanoseconds: UInt64(analysisTime * 1_000_000_000))
        
        // Analyze and extract relevant content
        let responseText = analyzeAndExtract(prompt: prompt, context: context)
        
        let totalTime = Date().timeIntervalSince(startTime)
        let tokensGenerated = responseText.split(separator: " ").count
        
        return LLMResponse(
            text: responseText,
            tokensGenerated: tokensGenerated,
            timeToFirstToken: 0.1,
            totalTime: totalTime
        )
    }
    
    private func analyzeAndExtract(prompt: String, context: String?) -> String {
        guard let context = context, !context.isEmpty else {
            return """
            I don't have any documents loaded yet. Please add documents to your library so I can analyze and extract information.
            
            [On-Device Analysis Ready - No AI model required]
            """
        }
        
        return extractRelevantInformation(query: prompt, retrievedContent: context)
    }
    
    private func extractRelevantInformation(query: String, retrievedContent: String) -> String {
        // 1. Analyze query intent
        let queryAnalysis = analyzeQuery(query)
        
        // 2. Analyze retrieved context
        let contextAnalysis = analyzeContext(retrievedContent)
        
        // 3. Find and rank relevant sentences
        let relevantInfo = findRelevantSentences(
            queryType: queryAnalysis.type,
            queryKeywords: queryAnalysis.keywords,
            queryEntities: queryAnalysis.entities,
            contextSentences: contextAnalysis.sentences,
            contextEntities: contextAnalysis.entities
        )
        
        // 4. Build structured response
        return buildExtractiveResponse(
            query: query,
            queryType: queryAnalysis.type,
            relevantSentences: relevantInfo
        )
    }

    
    private func analyzeQuery(_ query: String) -> QueryAnalysis {
        var type: QueryType = .general
        var keywords: Set<String> = []
        var entities: [String] = []
        
        let queryLower = query.lowercased()
        
        // Determine query type using pattern matching
        if queryLower.hasPrefix("what") || queryLower.contains("what is") || queryLower.contains("what are") {
            type = .definition
        } else if queryLower.hasPrefix("how") || queryLower.contains("how to") || queryLower.contains("how do") {
            type = .instruction
        } else if queryLower.hasPrefix("why") || queryLower.contains("why is") || queryLower.contains("why do") {
            type = .explanation
        } else if queryLower.hasPrefix("when") || queryLower.contains("when to") || queryLower.contains("when should") {
            type = .temporal
        } else if queryLower.hasPrefix("where") {
            type = .location
        } else if queryLower.contains("list") || queryLower.contains("tell me about") || queryLower.contains("describe") {
            type = .description
        }
        
        // Extract keywords using NLTagger
        tagger.string = query
        tagger.enumerateTags(in: query.startIndex..<query.endIndex, unit: .word, scheme: .lexicalClass) { tag, range in
            if let tag = tag {
                let word = String(query[range])
                // Keep nouns, verbs, adjectives
                if tag == .noun || tag == .verb || tag == .adjective {
                    if word.count > 2 {
                        keywords.insert(word.lowercased())
                    }
                }
            }
            return true
        }
        
        // Extract named entities
        tagger.enumerateTags(in: query.startIndex..<query.endIndex, unit: .word, scheme: .nameType) { tag, range in
            if let tag = tag {
                let entity = String(query[range])
                if tag == .personalName || tag == .placeName || tag == .organizationName {
                    entities.append(entity)
                }
            }
            return true
        }
        
        return QueryAnalysis(type: type, keywords: keywords, entities: entities)
    }
    
    private func analyzeContext(_ context: String) -> ContextAnalysis {
        var sentences: [SentenceInfo] = []
        var entities: [String] = []
        
        // Split into sentences using NLTokenizer
        let tokenizer = NLTokenizer(unit: .sentence)
        tokenizer.string = context
        tokenizer.enumerateTokens(in: context.startIndex..<context.endIndex) { range, _ in
            let sentence = String(context[range]).trimmingCharacters(in: .whitespacesAndNewlines)
            if sentence.count > 20 {
                // Analyze sentence
                let keywords = extractKeywords(from: sentence)
                let importance = calculateImportance(sentence)
                
                sentences.append(SentenceInfo(
                    text: sentence,
                    keywords: keywords,
                    importance: importance
                ))
            }
            return true
        }
        
        // Extract named entities from context
        tagger.string = context
        tagger.enumerateTags(in: context.startIndex..<context.endIndex, unit: .word, scheme: .nameType) { tag, range in
            if let tag = tag {
                let entity = String(context[range])
                if tag == .personalName || tag == .placeName || tag == .organizationName {
                    entities.append(entity)
                }
            }
            return true
        }
        
        return ContextAnalysis(sentences: sentences, entities: entities)
    }
    
    private func extractKeywords(from text: String) -> Set<String> {
        var keywords: Set<String> = []
        
        tagger.string = text
        tagger.enumerateTags(in: text.startIndex..<text.endIndex, unit: .word, scheme: .lexicalClass) { tag, range in
            if let tag = tag {
                let word = String(text[range])
                if (tag == .noun || tag == .verb || tag == .adjective) && word.count > 3 {
                    keywords.insert(word.lowercased())
                }
            }
            return true
        }
        
        return keywords
    }
    
    private func calculateImportance(_ sentence: String) -> Double {
        var score = 0.0
        
        // Longer sentences are often more informative
        score += Double(sentence.count) / 500.0
        
        // Sentences with numbers/data are important
        if sentence.range(of: #"\d+"#, options: .regularExpression) != nil {
            score += 0.3
        }
        
        // Sentences with key indicator words
        let importantWords = ["important", "must", "should", "warning", "note", "key", "essential", "critical"]
        for word in importantWords {
            if sentence.lowercased().contains(word) {
                score += 0.2
                break
            }
        }
        
        return min(score, 1.0)
    }
    
    private func findRelevantSentences(
        queryType: QueryType,
        queryKeywords: Set<String>,
        queryEntities: [String],
        contextSentences: [SentenceInfo],
        contextEntities: [String]
    ) -> [SentenceInfo] {
        
        var scoredSentences: [(sentence: SentenceInfo, score: Double)] = []
        
        for sentence in contextSentences {
            var score = sentence.importance
            
            // Keyword matching
            let matchingKeywords = sentence.keywords.intersection(queryKeywords)
            score += Double(matchingKeywords.count) * 0.5
            
            // Entity matching
            for entity in queryEntities {
                if sentence.text.contains(entity) {
                    score += 0.8
                }
            }
            
            // Bonus for query type relevance
            switch queryType {
            case .instruction:
                if sentence.text.contains("step") || sentence.text.contains("first") || 
                   sentence.text.lowercased().contains("to ") {
                    score += 0.3
                }
            case .definition:
                if sentence.text.contains("is") || sentence.text.contains("are") ||
                   sentence.text.contains("means") {
                    score += 0.3
                }
            case .explanation:
                if sentence.text.contains("because") || sentence.text.contains("due to") ||
                   sentence.text.contains("reason") {
                    score += 0.3
                }
            default:
                break
            }
            
            scoredSentences.append((sentence, score))
        }
        
        // Sort by score and return top results
        scoredSentences.sort { $0.score > $1.score }
        return scoredSentences.prefix(5).map { $0.sentence }
    }
    
    private func buildExtractiveResponse(
        query: String,
        queryType: QueryType,
        relevantSentences: [SentenceInfo]
    ) -> String {
        
        guard !relevantSentences.isEmpty else {
            return """
            I found your documents but couldn't identify specific information matching your query. Try rephrasing your question or asking about general topics covered in your documents.
            
            [On-Device Analysis - No matches found]
            """
        }
        
        // Generate introduction based on query type
        let intro: String
        switch queryType {
        case .definition:
            intro = "Based on your documents, here's what I found:"
        case .instruction:
            intro = "According to your documents:"
        case .explanation:
            intro = "Your documents explain:"
        case .temporal:
            intro = "Regarding timing, your documents state:"
        case .location:
            intro = "Regarding location, here's what your documents say:"
        case .description:
            intro = "From your documents:"
        case .general:
            intro = "Here's what I found in your documents:"
        }
        
        // Combine top sentences into coherent response
        let mainContent = relevantSentences
            .map { $0.text }
            .joined(separator: "\n\n")
        
        // Add footer explaining this is extractive, not generative
        let footer = "\n\n[On-Device Analysis: Extracted \(relevantSentences.count) relevant passages from your documents]"
        
        return intro + "\n\n" + mainContent + footer
    }
    
    // MARK: - Helper Types
    
    private enum QueryType {
        case definition      // "What is..."
        case instruction     // "How to..."
        case explanation     // "Why..."
        case temporal        // "When..."
        case location        // "Where..."
        case description     // "Tell me about...", "List..."
        case general         // Other
    }
    
    private struct QueryAnalysis {
        let type: QueryType
        let keywords: Set<String>
        let entities: [String]
    }
    
    private struct ContextAnalysis {
        let sentences: [SentenceInfo]
        let entities: [String]
    }
    
    private struct SentenceInfo {
        let text: String
        let keywords: Set<String>
        let importance: Double
    }
}

// MARK: - Apple Intelligence ChatGPT Extension (iOS 18.1+)
// REAL Apple Intelligence API - Uses Apple's built-in ChatGPT integration
// Requires iOS 18.1+, user must enable ChatGPT in Settings > Apple Intelligence & Siri
// NO OpenAI account required, free tier available, user consents per request

import AppIntents

@available(iOS 18.1, *)
class AppleChatGPTExtensionService: LLMService {
    
    var isAvailable: Bool {
        // Check if ChatGPT extension is enabled in system settings
        // User must enable: Settings > Apple Intelligence & Siri > ChatGPT
        return checkChatGPTAvailability()
    }
    
    var modelName: String { "ChatGPT (via Apple Intelligence)" }
    
    init() {
        if isAvailable {
            print("âœ… ChatGPT Extension available")
            print("   ðŸ” User consent required per request")
            print("   ðŸŒ Powered by OpenAI via Apple Intelligence")
        } else {
            print("âš ï¸  ChatGPT Extension not available")
            print("   ðŸ’¡ Enable in Settings > Apple Intelligence & Siri > ChatGPT")
        }
    }
    
    func generate(prompt: String, context: String?, config: InferenceConfig) async throws -> LLMResponse {
        guard isAvailable else {
            throw LLMError.modelUnavailable
        }
        
        let startTime = Date()
        
        // Construct augmented prompt with RAG context
        let fullPrompt: String
        if let context = context, !context.isEmpty {
            fullPrompt = """
            Context from user's documents:
            \(context)
            
            User question: \(prompt)
            
            Please answer based on the provided context.
            """
        } else {
            fullPrompt = prompt
        }
        
        // Use App Intents to send request to ChatGPT via Apple Intelligence
        // This triggers the system consent dialog automatically
        let response = try await sendChatGPTRequest(fullPrompt)
        
        let totalTime = Date().timeIntervalSince(startTime)
        let tokensGenerated = response.split(separator: " ").count
        
        return LLMResponse(
            text: response,
            tokensGenerated: tokensGenerated,
            timeToFirstToken: nil,
            totalTime: totalTime
        )
    }
    
    private func checkChatGPTAvailability() -> Bool {
        // Check if user has enabled ChatGPT in Apple Intelligence settings
        // This is a system-level setting, not app-specific
        // TODO: Replace with actual ChatGPT availability check when API is fully documented
        // For now, assume available on iOS 18.1+
        return true
    }
    
    private func sendChatGPTRequest(_ prompt: String) async throws -> String {
        // IMPLEMENTATION NOTE:
        // Apple's ChatGPT Extension API uses App Intents framework
        // The exact API is not fully public yet, but the pattern is:
        
        // 1. Create ChatGPT intent (system will show consent dialog)
        // 2. System handles the API call to OpenAI
        // 3. Response returned to app
        
        // Placeholder for actual implementation:
        throw LLMError.notImplemented
        
        /* Expected implementation pattern (when API is fully available):
        
        let intent = ChatGPTQueryIntent()
        intent.query = prompt
        
        // System shows consent dialog automatically
        // User can choose: "Allow Once", "Allow Always", or "Don't Allow"
        let result = try await intent.perform()
        
        guard let response = result.response else {
            throw LLMError.generationFailed("No response from ChatGPT")
        }
        
        return response.text
        */
    }
}

// MARK: - Core ML Implementation (Pathway B1)
// For custom models converted to .mlpackage format

import CoreML

class CoreMLLLMService: LLMService {
    
    private var model: MLModel?
    private let _modelName: String
    
    var isAvailable: Bool {
        return model != nil
    }
    
    var modelName: String {
        return _modelName
    }
    
    init(modelURL: URL) {
        self._modelName = modelURL.deletingPathExtension().lastPathComponent
        
        do {
            // Load Core ML model package
            let configuration = MLModelConfiguration()
            configuration.computeUnits = .all // Use CPU, GPU, and Neural Engine
            
            self.model = try MLModel(contentsOf: modelURL, configuration: configuration)
            
            print("âœ“ Successfully loaded Core ML model: \(_modelName)")
        } catch {
            print("âœ— Failed to load Core ML model: \(error.localizedDescription)")
            self.model = nil
        }
    }
    
    func generate(prompt: String, context: String?, config: InferenceConfig) async throws -> LLMResponse {
        guard let model = model else {
            throw LLMError.modelUnavailable
        }
        
        let startTime = Date()
        
        // Construct augmented prompt
        let augmentedPrompt: String
        if let context = context, !context.isEmpty {
            augmentedPrompt = """
            Context: \(context)
            
            Question: \(prompt)
            
            Answer:
            """
        } else {
            augmentedPrompt = prompt
        }
        
        // Tokenize input (this is model-specific and would need a proper tokenizer)
        // For now, this is a placeholder - in production, you'd use the model's tokenizer
        let inputTokens = tokenize(augmentedPrompt)
        
        // Create input feature provider
        let inputFeatures = try createInputFeatures(tokens: inputTokens, config: config)
        
        // Perform inference
        let prediction = try await model.prediction(from: inputFeatures)
        
        // Decode output tokens (model-specific)
        let outputText = try decodeOutput(prediction)
        
        let totalTime = Date().timeIntervalSince(startTime)
        
        return LLMResponse(
            text: outputText,
            tokensGenerated: outputText.split(separator: " ").count, // Rough estimate
            timeToFirstToken: nil, // Core ML doesn't stream by default
            totalTime: totalTime
        )
    }
    
    // MARK: - Tokenization (Placeholder)
    
    private func tokenize(_ text: String) -> [Int] {
        // This is a placeholder - in production, use the model's specific tokenizer
        // For real implementation, integrate swift-transformers or similar
        return text.split(separator: " ").map { _ in Int.random(in: 0..<50000) }
    }
    
    private func createInputFeatures(tokens: [Int], config: InferenceConfig) throws -> MLFeatureProvider {
        // Create MLMultiArray for input tokens
        // This is model-specific and depends on the model's input signature
        throw LLMError.notImplemented
    }
    
    private func decodeOutput(_ prediction: MLFeatureProvider) throws -> String {
        // Decode output tokens back to text
        // This is model-specific and depends on the model's output signature
        throw LLMError.notImplemented
    }
}

// MARK: - OpenAI Direct API (User's Own API Key)
// Direct integration with OpenAI API - bypasses Apple Intelligence
// User provides their own OpenAI API key
// Supports all OpenAI models: GPT-4o, GPT-4o-mini, GPT-4-turbo, etc.

class OpenAILLMService: LLMService {
    private let apiKey: String
    private let model: String
    private let endpoint = "https://api.openai.com/v1/chat/completions"
    
    var isAvailable: Bool { !apiKey.isEmpty }
    var modelName: String { model }
    
    init(apiKey: String, model: String = "gpt-4o-mini") {
        self.apiKey = apiKey
        self.model = model
        
        if isAvailable {
            print("âœ… OpenAI Direct API initialized")
            print("   ðŸ”‘ Using model: \(model)")
            print("   ðŸŒ Direct connection to OpenAI (not via Apple)")
        }
    }
    
    func generate(prompt: String, context: String?, config: InferenceConfig) async throws -> LLMResponse {
        guard !apiKey.isEmpty else {
            throw LLMError.modelUnavailable
        }
        
        let startTime = Date()
        
        // Construct messages with system prompt and user query
        var messages: [[String: String]] = [
            [
                "role": "system",
                "content": """
                You are a helpful AI assistant with access to documents. When provided with context \
                from documents, use that information to answer questions accurately and concisely. \
                If the context doesn't contain relevant information, say so clearly. Always be helpful \
                and conversational.
                """
            ]
        ]
        
        // Add context and user query
        if let context = context, !context.isEmpty {
            messages.append([
                "role": "user",
                "content": """
                Here is relevant information from the documents:
                
                \(context)
                
                Based on this information, please answer: \(prompt)
                """
            ])
        } else {
            messages.append([
                "role": "user",
                "content": prompt
            ])
        }
        
        // Prepare request body
        let requestBody: [String: Any] = [
            "model": model,
            "messages": messages,
            "temperature": Double(config.temperature),
            "max_tokens": config.maxTokens
        ]
        
        guard let jsonData = try? JSONSerialization.data(withJSONObject: requestBody) else {
            throw LLMError.generationFailed("Failed to encode request")
        }
        
        // Create request
        var request = URLRequest(url: URL(string: endpoint)!)
        request.httpMethod = "POST"
        request.setValue("Bearer \(apiKey)", forHTTPHeaderField: "Authorization")
        request.setValue("application/json", forHTTPHeaderField: "Content-Type")
        request.httpBody = jsonData
        
        // Make API call
        let (data, response) = try await URLSession.shared.data(for: request)
        
        guard let httpResponse = response as? HTTPURLResponse else {
            throw LLMError.generationFailed("Invalid response")
        }
        
        guard httpResponse.statusCode == 200 else {
            let errorMessage = String(data: data, encoding: .utf8) ?? "Unknown error"
            throw LLMError.generationFailed("API error (\(httpResponse.statusCode)): \(errorMessage)")
        }
        
        // Parse response
        guard let json = try? JSONSerialization.jsonObject(with: data) as? [String: Any],
              let choices = json["choices"] as? [[String: Any]],
              let firstChoice = choices.first,
              let message = firstChoice["message"] as? [String: Any],
              let content = message["content"] as? String else {
            throw LLMError.generationFailed("Failed to parse response")
        }
        
        // Extract usage statistics
        let tokensGenerated: Int
        if let usage = json["usage"] as? [String: Any],
           let completionTokens = usage["completion_tokens"] as? Int {
            tokensGenerated = completionTokens
        } else {
            tokensGenerated = content.split(separator: " ").count
        }
        
        let totalTime = Date().timeIntervalSince(startTime)
        
        return LLMResponse(
            text: content,
            tokensGenerated: tokensGenerated,
            timeToFirstToken: nil,
            totalTime: totalTime
        )
    }
}

// MARK: - REMOVED: Mock Implementation
// Mock services removed - use real implementations only:
// 1. OnDeviceAnalysisService - extractive QA with NaturalLanguage framework
// 2. AppleChatGPTExtensionService - Apple's ChatGPT integration (iOS 18.1+)
// 3. OpenAILLMService - direct OpenAI API with user's key
// 4. CoreMLLLMService - custom models (optional enhancement)

// MARK: - Errors

enum LLMError: LocalizedError {
    case modelUnavailable
    case generationFailed(String)
    case notImplemented
    
    var errorDescription: String? {
        switch self {
        case .modelUnavailable:
            return "LLM model is not available on this device"
        case .generationFailed(let message):
            return "Text generation failed: \(message)"
        case .notImplemented:
            return "Feature not yet implemented"
        }
    }
}
